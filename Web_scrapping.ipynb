{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T14:27:56.431295Z",
     "start_time": "2019-05-29T14:27:56.087466Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "app_id = 'd1f4cfd5-2a86-4c76-8593-5f999bf1a02d' \n",
    "api_key = 'MLXMoaTgjEozDqGUPJbWBsYQoKCwZ06n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T14:51:54.817549Z",
     "start_time": "2019-05-29T14:51:54.804526Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyjq\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def get_text():\n",
    "    app_id = 'd1f4cfd5-2a86-4c76-8593-5f999bf1a02d' \n",
    "    api_key = 'MLXMoaTgjEozDqGUPJbWBsYQoKCwZ06n'\n",
    "    \n",
    "    data_dict = []\n",
    "    for i in range(1,11):\n",
    "        api_key = 'MLXMoaTgjEozDqGUPJbWBsYQoKCwZ06n'\n",
    "        url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?q=s&n500&page='+str(i)+'&api-key=' +api_key\n",
    "        r = requests.get(url, timeout=10)\n",
    "        print('requesting page ', i)\n",
    "        data = r.json()\n",
    "        data_dict.append(data)\n",
    "    \n",
    "    data_list = []\n",
    "    for i in data_dict:\n",
    "        jq_query = f'.response .docs [] | {{url: .web_url, snippet: .snippet, date: .pub_date}}'\n",
    "        output = pyjq.all(jq_query, i)\n",
    "        data_list.extend(output)\n",
    "        print(data_list)\n",
    "        print(len(data_list))\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df.drop_duplicates(keep=False, inplace=True)\n",
    "    df['date'] = df['date'].map(lambda i: str(i)[:10])\n",
    "    df['date'] = df['date'].map(lambda i: datetime.datetime.strptime(i, '%Y-%m-%d'))\n",
    "    df.sort_values('date', inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:48:08.418414Z",
     "start_time": "2019-05-29T15:48:02.065294Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = []\n",
    "for i in range(1,11):\n",
    "    api_key = 'MLXMoaTgjEozDqGUPJbWBsYQoKCwZ06n'\n",
    "    url = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?q=s&n500&page='+str(i)+'&api-key=' +api_key\n",
    "    r = requests.get(url, timeout=10)\n",
    "    print('requesting page ', i)\n",
    "    data = r.json()\n",
    "    data_dict.append(data)\n",
    "    print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:48:19.425021Z",
     "start_time": "2019-05-29T15:48:19.135448Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in data_dict:\n",
    "    jq_query = f'.response .docs [] | {{url: .web_url, snippet: .snippet, date: .pub_date}}'\n",
    "    output = pyjq.all(jq_query, i)\n",
    "    data_list.extend(output)\n",
    "    print(data_list)\n",
    "    print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:49:31.554525Z",
     "start_time": "2019-05-29T15:49:31.536480Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "df.drop_duplicates(keep=False, inplace=True)\n",
    "df['date'] = df['date'].map(lambda i: str(i)[:10])\n",
    "df['date'] = df['date'].map(lambda i: datetime.datetime.strptime(i, '%Y-%m-%d'))\n",
    "df.sort_values('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:49:33.054409Z",
     "start_time": "2019-05-29T15:49:32.998363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:51:34.496929Z",
     "start_time": "2019-05-29T15:51:34.468044Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "article_dict = df.to_dict(orient='records')\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping each article with URL from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:51:50.829802Z",
     "start_time": "2019-05-29T15:51:50.655078Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "\n",
    "def get_nyt_text(url):\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "    page = requests.get(url, headers=headers,timeout=5)\n",
    "    page.status_code\n",
    "    \n",
    "    soup = BS(page.content, 'html.parser')\n",
    "    content = soup.findAll('p', class_ = 'css-18icg9x evys1bk0')\n",
    "    \n",
    "    nyt = ''\n",
    "    for index in range(len(content)):\n",
    "        nyt += content[index].get_text()\n",
    "        \n",
    "    return nyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:53:57.615460Z",
     "start_time": "2019-05-29T15:53:40.279329Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in article_dict:\n",
    "    i.update({'article':get_nyt_text(i['url'])})\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:54:18.450558Z",
     "start_time": "2019-05-29T15:54:18.443574Z"
    }
   },
   "outputs": [],
   "source": [
    "len(article_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Vader to evaluate sentiment of each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:54:59.500380Z",
     "start_time": "2019-05-29T15:54:57.515720Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "si = SentimentIntensityAnalyzer()\n",
    "\n",
    "for i in article_dict:\n",
    "    i.update({'sentiment':si.polarity_scores(i['article'])})\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:58:29.703600Z",
     "start_time": "2019-05-29T15:58:29.636201Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in article_dict:\n",
    "    i.update({'compound': i['sentiment']['compound']})\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:59:00.325879Z",
     "start_time": "2019-05-29T15:59:00.225968Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sentiment = pd.DataFrame(article_dict)\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T15:59:41.049033Z",
     "start_time": "2019-05-29T15:59:41.010976Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sentiment.drop(columns=['article', 'sentiment', 'url', 'snippet'], inplace=True)\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T16:00:09.380738Z",
     "start_time": "2019-05-29T16:00:09.359158Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "daily_sentiment = df_sentiment.groupby('date').mean()\n",
    "daily_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:17:21.233310Z",
     "start_time": "2019-05-29T17:17:21.193115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA = pd.read_csv('TSLA.csv')\n",
    "TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:17:21.514025Z",
     "start_time": "2019-05-29T17:17:21.490128Z"
    }
   },
   "outputs": [],
   "source": [
    "TSLA.drop(columns=['High', 'Low', 'Adj Close', 'Volume'], inplace=True)\n",
    "TSLA['Date'] = TSLA['Date'].map(lambda i: datetime.datetime.strptime(i, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:17:22.219877Z",
     "start_time": "2019-05-29T17:17:21.751708Z"
    }
   },
   "outputs": [],
   "source": [
    "TSLA['Daily_Change'] = TSLA['Close'] / TSLA['Open'] -1\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "TSLA.plot(figsize=(12,8), x='Date', y='Daily_Change', title= 's&n500 Daily Performance', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:17:22.644382Z",
     "start_time": "2019-05-29T17:17:22.224921Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TSLA.plot(figsize=(12,8), x='Date', y='Close', title= 's&n500 Daily Close', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:17:22.670916Z",
     "start_time": "2019-05-29T17:17:22.649110Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA.set_index('Date', inplace=True)\n",
    "TSLA.drop(columns=['Open', 'Daily_Change'], inplace=True)\n",
    "TSLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:17:30.208694Z",
     "start_time": "2019-05-29T17:17:29.025555Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "decomposition = seasonal_decompose(TSLA, freq=12)\n",
    "fig = plt.figure()\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(15, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:22:37.422526Z",
     "start_time": "2019-05-29T17:22:36.852870Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(TSLA.Close.iloc[1:], lags=18, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(TSLA.Close.iloc[1:], lags=18, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:24:02.072867Z",
     "start_time": "2019-05-29T17:24:01.618470Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries, window):\n",
    "    \n",
    "    rolmean = timeseries.rolling(window=window).mean()\n",
    "    rolstd = timeseries.rolling(window=window).std()\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries.iloc[window:], color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    print ('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print (dfoutput)\n",
    "\n",
    "test_stationarity(TSLA.Close, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:30:06.616398Z",
     "start_time": "2019-05-29T17:30:06.264326Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "TSLA['natural_log'] = TSLA.Close.apply(lambda x: np.log(x))  \n",
    "test_stationarity(TSLA.natural_log, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:33:46.339185Z",
     "start_time": "2019-05-29T17:33:45.836680Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(TSLA.natural_log.iloc[1:], lags=18, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(TSLA.natural_log.iloc[1:], lags=18, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:40:27.831818Z",
     "start_time": "2019-05-29T17:40:27.446941Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "TSLA['first_difference'] = TSLA.Close - TSLA.Close.shift(1)  \n",
    "test_stationarity(TSLA.first_difference.dropna(inplace=False),12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:40:28.919916Z",
     "start_time": "2019-05-29T17:40:28.465854Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(TSLA.first_difference.iloc[1:], lags=18, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(TSLA.first_difference.iloc[1:], lags=18, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:40:34.325885Z",
     "start_time": "2019-05-29T17:40:33.935191Z"
    }
   },
   "outputs": [],
   "source": [
    "TSLA['seasonal_difference'] = TSLA.Close - TSLA.Close.shift(12)  \n",
    "test_stationarity(TSLA.seasonal_difference.dropna(inplace=False), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:40:36.014891Z",
     "start_time": "2019-05-29T17:40:35.551758Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(TSLA.seasonal_difference.iloc[13:], lags=24, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(TSLA.seasonal_difference.iloc[13:], lags=24, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:40:16.239371Z",
     "start_time": "2019-05-29T17:40:15.876871Z"
    }
   },
   "outputs": [],
   "source": [
    "TSLA['seasonal_first_difference'] = TSLA.first_difference - TSLA.first_difference.shift(12)  \n",
    "test_stationarity(TSLA.seasonal_first_difference.dropna(inplace=False), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:05.877703Z",
     "start_time": "2019-05-29T17:35:05.427061Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(TSLA.seasonal_first_difference.iloc[13:], lags=24, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(TSLA.seasonal_first_difference.iloc[13:], lags=24, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:39.782562Z",
     "start_time": "2019-05-29T17:35:39.377651Z"
    }
   },
   "outputs": [],
   "source": [
    "TSLA['log_first_difference'] = TSLA.natural_log - TSLA.natural_log.shift(1)  \n",
    "TSLA['log_seasonal_first_difference'] = TSLA.log_first_difference - TSLA.log_first_difference.shift(12)  \n",
    "\n",
    "test_stationarity(TSLA.log_seasonal_first_difference.dropna(inplace=False), 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:35:47.875673Z",
     "start_time": "2019-05-29T17:35:47.429373Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(TSLA.log_seasonal_first_difference.iloc[13:], lags=24, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(TSLA.log_seasonal_first_difference.iloc[13:], lags=24, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining data with sentiment compound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:55:57.634807Z",
     "start_time": "2019-05-29T17:55:57.612539Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_FD = pd.DataFrame(TSLA.first_difference)\n",
    "TSLA_FD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T17:56:55.233150Z",
     "start_time": "2019-05-29T17:56:55.199297Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_FD_COMP = daily_sentiment.merge(TSLA_FD, left_index=True, right_index=True)\n",
    "TSLA_FD_COMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:01:59.605095Z",
     "start_time": "2019-05-29T18:01:59.183808Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "TSLA_FD_COMP.plot(ax=ax1, y='first_difference', use_index=True, figsize=(10,7), legend=True, c='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "TSLA_FD_COMP.plot(ax=ax2, y='compound', use_index=True, figsize=(10,7), legend=True)\n",
    "\n",
    "ax1.set_ylabel(\"first_difference\", fontsize=10)\n",
    "ax1.set_xlabel(\"compound\", fontsize=10)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:06:28.693503Z",
     "start_time": "2019-05-29T18:06:28.395379Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_FD_COMP.plot.scatter(x='compound', y='first_difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining data with sentiment elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:08:58.629681Z",
     "start_time": "2019-05-29T18:08:58.565328Z"
    }
   },
   "outputs": [],
   "source": [
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:10:30.122564Z",
     "start_time": "2019-05-29T18:10:30.046209Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in article_dict:\n",
    "    i.update({'positive': i['sentiment']['pos']})\n",
    "    i.update({'negative': i['sentiment']['neg']})\n",
    "    i.update({'neutral': i['sentiment']['neu']})\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:10:56.622387Z",
     "start_time": "2019-05-29T18:10:56.529000Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiments = pd.DataFrame(article_dict)\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:11:28.705950Z",
     "start_time": "2019-05-29T18:11:28.699070Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiments.drop(columns=['article', 'compound', 'sentiment', 'snippet', 'url'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:12:51.668088Z",
     "start_time": "2019-05-29T18:12:51.641072Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiments = sentiments.groupby('date').mean()\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:12:55.572993Z",
     "start_time": "2019-05-29T18:12:55.541273Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_FD_SENT = sentiments.merge(TSLA_FD, left_index=True, right_index=True)\n",
    "TSLA_FD_SENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:15:04.681465Z",
     "start_time": "2019-05-29T18:15:04.228454Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "\n",
    "TSLA_FD_SENT.plot(ax=ax1, y='first_difference', use_index=True, figsize=(10,7), legend=True, c='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "TSLA_FD_SENT.plot(ax=ax2, y='negative', use_index=True, figsize=(10,7), legend=True)\n",
    "TSLA_FD_SENT.plot(ax=ax2, y='neutral', use_index=True, figsize=(10,7), legend=True)\n",
    "TSLA_FD_SENT.plot(ax=ax2, y='positive', use_index=True, figsize=(10,7), legend=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T18:17:11.335400Z",
     "start_time": "2019-05-29T18:17:10.646944Z"
    }
   },
   "outputs": [],
   "source": [
    "TSLA_FD_SENT.plot.scatter(x='positive', y='first_difference')\n",
    "TSLA_FD_SENT.plot.scatter(x='negative', y='first_difference')\n",
    "TSLA_FD_SENT.plot.scatter(x='neutral', y='first_difference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting target variable into binary data - Vader for snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T14:59:42.031928Z",
     "start_time": "2019-05-30T14:59:41.850108Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('nyt_api_2010_2019.json') as f:\n",
    "    d = json.load(f)\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:31:45.421638Z",
     "start_time": "2019-05-30T17:31:45.252185Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment = pd.read_pickle(\"sentiment_counts.pkl\")\n",
    "\n",
    "sentiment.index = pd.DatetimeIndex(sentiment.index)\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:32:42.291968Z",
     "start_time": "2019-05-30T17:32:42.254231Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_DF = pd.read_csv('TSLA 2010.csv')\n",
    "TSLA_DF['First_Diff'] = TSLA_DF.Close - TSLA_DF.Close.shift(1)\n",
    "TSLA_DF['Date'] = pd.to_datetime(TSLA_DF.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:32:43.069119Z",
     "start_time": "2019-05-30T17:32:43.052038Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_sentiment = sentiment.merge(TSLA_DF, left_index=True, right_on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:32:43.688181Z",
     "start_time": "2019-05-30T17:32:43.675628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_sentiment['Movement'] = np.where(TSLA_sentiment['First_Diff'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:32:55.505321Z",
     "start_time": "2019-05-30T17:32:55.359172Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TSLA_sentiment_Cleaned = TSLA_sentiment[TSLA_sentiment['VADER snippet compound'] != 0]\n",
    "TSLA_sentiment_Cleaned.set_index('Date', inplace=True)\n",
    "TSLA_sentiment_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:33:03.985690Z",
     "start_time": "2019-05-30T17:33:03.907971Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Nasdaq = pd.read_csv('Nasdaq2010-2019.csv')\n",
    "Nasdaq['Nasdaq_First_Diff'] = Nasdaq.Close - Nasdaq.Close.shift(1)\n",
    "Nasdaq['Nasdaq_Movement'] = np.where(Nasdaq['Nasdaq_First_Diff'] > 0, 1, 0)\n",
    "Nasdaq['Date'] = pd.to_datetime(Nasdaq.Date)\n",
    "Nasdaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:33:09.026690Z",
     "start_time": "2019-05-30T17:33:08.898010Z"
    }
   },
   "outputs": [],
   "source": [
    "Combined = TSLA_sentiment_Cleaned.merge(Nasdaq, left_index=True, right_on='Date')\n",
    "Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:33:17.676054Z",
     "start_time": "2019-05-30T17:33:17.548567Z"
    }
   },
   "outputs": [],
   "source": [
    "Cleaned = Combined[Combined['VADER snippet compound'] != 0]\n",
    "Cleaned.set_index('Date', inplace=True)\n",
    "Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:33:35.724819Z",
     "start_time": "2019-05-30T17:33:35.712293Z"
    }
   },
   "outputs": [],
   "source": [
    "Cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T21:22:07.122170Z",
     "start_time": "2019-05-29T21:22:07.113969Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:03.765147Z",
     "start_time": "2019-05-30T17:34:03.755432Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Cleaned[['VADER snippet compound', 'VADER snippet neg', 'VADER snippet neu', 'VADER snippet pos', 'article count', 'Volume_x', 'Nasdaq_Movement']]\n",
    "y = Cleaned['Movement']\n",
    "\n",
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:05.069132Z",
     "start_time": "2019-05-30T17:34:05.059011Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:05.633560Z",
     "start_time": "2019-05-30T17:34:05.623059Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:06.722857Z",
     "start_time": "2019-05-30T17:34:06.707221Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train =pd.DataFrame(data=scaler.transform(X_train), columns=columns)\n",
    "X_test =pd.DataFrame(data=scaler.transform(X_test), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:07.402577Z",
     "start_time": "2019-05-30T17:34:07.385213Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "logreg.fit(X_train, y_train)\n",
    "dictionary = dict(zip(list(X_train.columns), list(logreg.coef_[0])))\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:09.517444Z",
     "start_time": "2019-05-30T17:34:09.503076Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:11.206445Z",
     "start_time": "2019-05-30T17:34:10.776653Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "classes = ['Moved Up', \"Moved Down\"]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "import itertools\n",
    "plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naives Baye model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:17.803979Z",
     "start_time": "2019-05-30T17:34:17.791667Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:18.323666Z",
     "start_time": "2019-05-30T17:34:18.312871Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:18.879623Z",
     "start_time": "2019-05-30T17:34:18.861539Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:19.599140Z",
     "start_time": "2019-05-30T17:34:19.586400Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:21.120515Z",
     "start_time": "2019-05-30T17:34:20.748343Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "classes = ['Moved Up', \"Moved Down\"]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "import itertools\n",
    "plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:21.977544Z",
     "start_time": "2019-05-30T17:34:21.967482Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Textblob for article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:44.833508Z",
     "start_time": "2019-05-30T17:34:44.738996Z"
    }
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sentiment_article = pd.read_pickle(\"sentiment_counts.pkl\")\n",
    "\n",
    "sentiment_article.index = pd.DatetimeIndex(sentiment_article.index)\n",
    "sentiment_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:34:48.192213Z",
     "start_time": "2019-05-30T17:34:48.066780Z"
    }
   },
   "outputs": [],
   "source": [
    "Combined_TB = sentiment_article.merge(Combined, left_index=True, right_on='Date')\n",
    "Combined_TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:35:05.404798Z",
     "start_time": "2019-05-30T17:35:05.376752Z"
    }
   },
   "outputs": [],
   "source": [
    "Cleaned_TB = Combined_TB[Combined_TB['TextBlob article polarity_x'] != 0]\n",
    "Cleaned_TB.set_index('Date', inplace=True)\n",
    "Cleaned_TB.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:17:31.800594Z",
     "start_time": "2019-05-31T13:17:31.777815Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Cleaned_TB[['TextBlob article polarity_x', 'TextBlob article subjectivity_x','article count_y',  'Volume_x', 'Nasdaq_Movement']]\n",
    "y = Cleaned_TB['Movement']\n",
    "\n",
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:17:32.665730Z",
     "start_time": "2019-05-31T13:17:32.588734Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train =pd.DataFrame(data=scaler.transform(X_train), columns=columns)\n",
    "X_test =pd.DataFrame(data=scaler.transform(X_test), columns=columns)\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "logreg.fit(X_train, y_train)\n",
    "dictionary = dict(zip(list(X_train.columns), list(logreg.coef_[0])))\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:17:34.700662Z",
     "start_time": "2019-05-31T13:17:34.688306Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)\n",
    "\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:17:37.088575Z",
     "start_time": "2019-05-31T13:17:35.745990Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "classes = ['Moved Up', \"Moved Down\"]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "import itertools\n",
    "plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:19:19.447795Z",
     "start_time": "2019-05-31T13:19:19.428376Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:19:20.159750Z",
     "start_time": "2019-05-31T13:19:20.147657Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:19:21.586802Z",
     "start_time": "2019-05-31T13:19:21.238043Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "classes = ['Moved Up', \"Moved Down\"]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "import itertools\n",
    "plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Vader and TextBlob as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:38:44.198091Z",
     "start_time": "2019-05-30T17:38:44.187431Z"
    }
   },
   "outputs": [],
   "source": [
    "X = Cleaned_TB[['VADER snippet compound_x', 'VADER snippet neg_x','VADER snippet neu_x', 'VADER snippet pos_x', 'article count_x', 'TextBlob article polarity_x', 'TextBlob article subjectivity_x', 'Volume_x', 'Nasdaq_Movement']]\n",
    "y = Cleaned_TB['Movement']\n",
    "\n",
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:38:45.169969Z",
     "start_time": "2019-05-30T17:38:45.131320Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=35)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train =pd.DataFrame(data=scaler.transform(X_train), columns=columns)\n",
    "X_test =pd.DataFrame(data=scaler.transform(X_test), columns=columns)\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "logreg.fit(X_train, y_train)\n",
    "dictionary = dict(zip(list(X_train.columns), list(logreg.coef_[0])))\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T17:38:46.093488Z",
     "start_time": "2019-05-30T17:38:46.081772Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)\n",
    "\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_class))\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:10:01.391611Z",
     "start_time": "2019-05-31T13:10:01.270756Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:10:09.654054Z",
     "start_time": "2019-05-31T13:10:09.635032Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred))\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T13:10:17.056071Z",
     "start_time": "2019-05-31T13:10:16.510189Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "classes = ['Moved Up', \"Moved Down\"]\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "import itertools\n",
    "plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
