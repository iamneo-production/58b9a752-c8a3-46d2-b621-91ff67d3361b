{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import app_id, api_key\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The New York Times: API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def call_nyt_by_year_and_page(year, page):\n",
    "    root_url   = 'https://api.nytimes.com/svc/search/v2/articlesearch.json?q=s&n500&sort=newest'\n",
    "    begin_date = '&begin_date={}0101'.format(str(year))\n",
    "    end_date   = '&end_date={}1231'.format(str(year))\n",
    "    pagination = '&page={}'.format(str(page))\n",
    "    doc_params = '&fl=web_url&fl=snippet&fl=pub_date&fl=_id&fl=lead_paragraph'\n",
    "    \n",
    "    url        = root_url + begin_date + end_date + pagination + doc_params + '&api-key=' + api_key\n",
    "    print(url)\n",
    "    \n",
    "    response   = requests.get(url)\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_nyt_by_year(year):\n",
    "    annual_articles = []\n",
    "    \n",
    "    total_results = call_nyt_by_year_and_page(year,0)\n",
    "    hits  = total_results['response']['meta']['hits']\n",
    "    \n",
    "    for i in range(int(hits/10)):\n",
    "        query = call_nyt_by_year_and_page(year,i)\n",
    "        annual_articles = annual_articles + query['response']['docs']\n",
    "        time.sleep(7)\n",
    "        \n",
    "    return annual_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request all pages of articles for all years\n",
    "# returns a list of article dictionaries\n",
    "def call_nyt_by_all_years(years):\n",
    "    all_articles = []\n",
    "    \n",
    "    for year in years:\n",
    "        annual_articles = call_nyt_by_year(year)\n",
    "        all_articles = all_articles + annual_articles\n",
    "        time.sleep(7)\n",
    "        \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_years    = list(range(2010,2020))\n",
    "all_articles = call_nyt_by_all_years(all_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nyt_api.json\", \"w\") as write_file:\n",
    "    json.dump(all_articles, write_file)\n",
    "    \n",
    "with open(\"nyt_api.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The New York Times: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nyt_text(url):\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36'}\n",
    "    page = requests.get(url, headers=headers,timeout=5)\n",
    "    page.status_code\n",
    "    \n",
    "    soup = BS(page.content, 'html.parser')\n",
    "    content = soup.findAll('p', class_ = 'css-18icg9x evys1bk0')\n",
    "    \n",
    "    nyt = ''\n",
    "    for index in range(len(content)):\n",
    "        nyt += content[index].get_text()\n",
    "        \n",
    "    return nyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    print(i)\n",
    "    print(data[i]['web_url'])\n",
    "    data[i]['article'] = get_nyt_text(data[i]['web_url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing: VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SentimentIntensityAnalyzer()\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i]['VADER snippet neg']      = si.polarity_scores(data[i]['snippet'])['neg']\n",
    "    data[i]['VADER snippet neu']      = si.polarity_scores(data[i]['snippet'])['neu']\n",
    "    data[i]['VADER snippet pos']      = si.polarity_scores(data[i]['snippet'])['pos']\n",
    "    data[i]['VADER snippet compound'] = si.polarity_scores(data[i]['snippet'])['compound']\n",
    "    \n",
    "    data[i]['VADER lead neg']      = si.polarity_scores(data[i]['lead_paragraph'])['neg']\n",
    "    data[i]['VADER lead neu']      = si.polarity_scores(data[i]['lead_paragraph'])['neu']\n",
    "    data[i]['VADER lead pos']      = si.polarity_scores(data[i]['lead_paragraph'])['pos']\n",
    "    data[i]['VADER lead compound'] = si.polarity_scores(data[i]['lead_paragraph'])['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing: TextBlob Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data[i]['TextBlob snippet polarity']     = TextBlob(data[i]['snippet']).sentiment[0]\n",
    "    data[i]['TextBlob snippet subjectivity'] = TextBlob(data[i]['snippet']).sentiment[1]\n",
    "    \n",
    "    data[i]['TextBlob lead polarity']        = TextBlob(data[i]['lead_paragraph']).sentiment[0]\n",
    "    data[i]['TextBlob lead subjectivity']    = TextBlob(data[i]['lead_paragraph']).sentiment[1]\n",
    "    \n",
    "    data[i]['TextBlob article polarity']     = TextBlob(data[i]['article']).sentiment[0]\n",
    "    data[i]['TextBlob article subjectivity'] = TextBlob(data[i]['article']).sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nyt_api_and_articles_vader_textblob.json\", \"w\") as write_file:\n",
    "    json.dump(data, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning, Counting and Aggregating DataFrame Records by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_articles = 0\n",
    "full_articles = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    if len(data[i]['article']) == 0:\n",
    "        null_articles += 1\n",
    "    else:\n",
    "        full_articles += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = pd.DataFrame(data)\n",
    "sentiment['date'] = pd.to_datetime(sentiment.pub_date).dt.date\n",
    "sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_clean = sentiment[sentiment['TextBlob article subjectivity'] != 0]\n",
    "sentiment_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_clean = sentiment_clean.groupby('date').mean()\n",
    "sentiment_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = sentiment_clean.groupby('date').count()\n",
    "counts['article count'] = counts['_id']\n",
    "counts = counts[['article count']]\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = sentiment_clean.merge(counts, left_on='date', right_index=True)\n",
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts.to_pickle(\"sentiment_counts.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final DataFrame with Sentiment Analysis by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = pd.read_pickle(\"sentiment_counts.pkl\")\n",
    "sentiment_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "axes = sentiment_counts.hist(figsize=(14,14))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
